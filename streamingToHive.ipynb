{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, LongType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"sentiment_analysis_read\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Kafka details\n",
        "kafka_server = \"kafka-broker:29092\"\n",
        "topic = \"sentiment_analysis\"\n",
        "\n",
        "# Define the schema to match your data\n",
        "schema = StructType([\n",
        "    StructField(\"text\", StringType()),\n",
        "    StructField(\"date\", StringType()),\n",
        "    StructField(\"likes\", DoubleType()),\n",
        "    StructField(\"is_retweet\", BooleanType()),\n",
        "    StructField(\"retweets\", LongType()),\n",
        "    StructField(\"country\", StringType()),\n",
        "])\n",
        "\n",
        "# Read data from Kafka\n",
        "df = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
        "    .option(\"subscribe\", topic) \\\n",
        "    .load() \\\n",
        "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
        "    .select(\"data.*\")\n",
        "\n",
        "# Write data to a Parquet format\n",
        "query = df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", \"/user/hive/warehouse/datatest2\") \\\n",
        "    .option(\"checkpointLocation\", \"/tmp2/check2\") \\\n",
        "    .start() \n",
        "\n",
        "# Await termination to keep the stream running\n",
        "query.awaitTermination()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "streaming"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
