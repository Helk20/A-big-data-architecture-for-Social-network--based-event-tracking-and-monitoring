{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "import subprocess\n",
        "\n",
        "# Packages to install\n",
        "packages_to_install = ['kafka-python', 'ntscraper']\n",
        "\n",
        "# Use subprocess to run the pip install commands\n",
        "for package in packages_to_install:\n",
        "    subprocess.call(['pip', 'install', package])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from ntscraper import Nitter\n",
        "\n",
        "# Define the Kafka Producer configuration\n",
        "KAFKA_BOOTSTRAP_SERVERS = ['kafka-broker:29092']\n",
        "KAFKA_TOPIC_NAME = 'sentiment_analysis'\n",
        "KAFKA_PRODUCER_CONFIG = {\n",
        "    'bootstrap_servers': KAFKA_BOOTSTRAP_SERVERS\n",
        "}\n",
        "\n",
        "terms = [\"genocide\", \"gaza\", \"world\"]\n",
        "continents = [\"Africa\", \"Asia\", \"Europe\", \"North America\", \"South America\"]\n",
        "\n",
        "def get_twitter_data(terms, continents):\n",
        "    Twitter_data_list = []\n",
        "    scraper = Nitter(0)\n",
        "\n",
        "    for term in terms:\n",
        "        for country in continents:\n",
        "            tweets = scraper.get_tweets(term, mode='term', language='en', number=100, near=country)\n",
        "            \n",
        "            # Print information about the tweets variable\n",
        "            # print(f\"Term: {term}, Country: {country}, Tweets: {tweets}\")\n",
        "            \n",
        "            for x in tweets['tweets']:\n",
        "                    data = {\n",
        "                        'text': x['text'],\n",
        "                        'date': x['date'],\n",
        "                        'likes': x['stats']['likes'],\n",
        "                        'is_retweet': x['is-retweet'],\n",
        "                        'retweets': x['stats']['retweets'],\n",
        "                        'country': country  # Add the country name to the data\n",
        "                    }\n",
        "                    Twitter_data_list.append(data)\n",
        "\n",
        "    return Twitter_data_list\n",
        "\n",
        "def main(terms):\n",
        "    try:\n",
        "        producer = KafkaProducer(**KAFKA_PRODUCER_CONFIG)\n",
        "\n",
        "        twitter_data = get_twitter_data(terms, continents)\n",
        "        for tweet in twitter_data:\n",
        "            json_data = json.dumps(tweet)\n",
        "            print(json_data)\n",
        "            producer.send(KAFKA_TOPIC_NAME, json_data.encode())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error: {e}')\n",
        "\n",
        "main(terms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "%python\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "KafkaStreaming"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
